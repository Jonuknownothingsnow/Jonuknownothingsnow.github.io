<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jonuknownothingsnow.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本来之前做NLP方面的预训练模型grpc服务的时候开始写这篇文章，后面因为一些事情去赶其他的了，现在重新补完, 有的东西可能因为比较久之前写的的有点不一样。由于我更多做K8S平台化、工程化的内容，本文对于模型结构方面只是做简单介绍，可能会在理解上有些出入。 文中的目前指2020年8月15日，大部分提到的模型都是19年底-20年初，对于最新业界进展可能没有及时跟进  [TOC]">
<meta property="og:type" content="article">
<meta property="og:title" content="谐门武学-NLP预训练模型服务分享">
<meta property="og:url" content="https://jonuknownothingsnow.github.io/2020/08/15/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%9C%8D%E5%8A%A1%E5%88%86%E4%BA%AB/index.html">
<meta property="og:site_name" content="火力支援">
<meta property="og:description" content="本来之前做NLP方面的预训练模型grpc服务的时候开始写这篇文章，后面因为一些事情去赶其他的了，现在重新补完, 有的东西可能因为比较久之前写的的有点不一样。由于我更多做K8S平台化、工程化的内容，本文对于模型结构方面只是做简单介绍，可能会在理解上有些出入。 文中的目前指2020年8月15日，大部分提到的模型都是19年底-20年初，对于最新业界进展可能没有及时跟进  [TOC]">
<meta property="og:locale">
<meta property="article:published_time" content="2020-08-14T16:00:00.000Z">
<meta property="article:modified_time" content="2020-08-18T06:31:54.701Z">
<meta property="article:author" content="王大炮">
<meta property="article:tag" content="Bert">
<meta property="article:tag" content="Albert">
<meta property="article:tag" content="Roberta">
<meta property="article:tag" content="预训练模型">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jonuknownothingsnow.github.io/2020/08/15/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%9C%8D%E5%8A%A1%E5%88%86%E4%BA%AB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>谐门武学-NLP预训练模型服务分享 | 火力支援</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火力支援</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://jonuknownothingsnow.github.io/2020/08/15/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%9C%8D%E5%8A%A1%E5%88%86%E4%BA%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="王大炮">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火力支援">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          谐门武学-NLP预训练模型服务分享
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-15 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-15T00:00:00+08:00">2020-08-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-18 14:31:54" itemprop="dateModified" datetime="2020-08-18T14:31:54+08:00">2020-08-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">算法模型</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>本来之前做NLP方面的预训练模型grpc服务的时候开始写这篇文章，后面因为一些事情去赶其他的了，现在重新补完, 有的东西可能因为比较久之前写的的有点不一样。由于我更多做K8S平台化、工程化的内容，本文对于模型结构方面只是做简单介绍，可能会在理解上有些出入。</p>
<p>文中的目前指2020年8月15日，大部分提到的模型都是19年底-20年初，对于最新业界进展可能没有及时跟进</p>
</blockquote>
<p>[TOC]</p>
<a id="more"></a>

<h2 id="什么是预训练模型"><a href="#什么是预训练模型" class="headerlink" title="什么是预训练模型"></a>什么是预训练模型</h2><p>预训练模型是相对于我们针对具体业务训练模型而言的，预训练模型一般是指在一个或多个任务上使用大量广泛的数据进行训练，从而使模型获得对领域内多种任务都具备一定的适用性。</p>
<p>通过使用预训练模型，我们可以降低对业务数据量的要求，同时获得较为可观的精度。</p>
<p>使用预训练模型主要有两种方式，一是基于训练好的预训练模型，在业务数据上进行训练微调。另一种是Bottleneck，去掉预训练模型的最后一层或数层，重新训练一个头模型拼接上，本质是使用预训练模型作为向量化工具。</p>
<h2 id="自然语言领域的预训练模型"><a href="#自然语言领域的预训练模型" class="headerlink" title="自然语言领域的预训练模型"></a>自然语言领域的预训练模型</h2><p>相比CV领域来说，NLP领域的预训练模型应用地比较晚，真正大规模的使用，还是在Bert出现之后（此前虽然也有Elmo等，但相对小众），Bert之后高性能的NLP预训练模型不断涌现，并在业内得到较为广泛的应用（例如美团在这方面就有比较多的实践，详见<a target="_blank" rel="noopener" href="https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html">这篇博客</a>)</p>
<h3 id="向量化问题"><a href="#向量化问题" class="headerlink" title="向量化问题"></a>向量化问题</h3><p>自然语言中一个重要的工作就是对文本的向量化，与图像天生就是数值矩阵不同，文本没有特别直接的向量化方法，目前主流的向量化方法是将文本进行分词之后，先做onehot处理，再将onehot向量嵌入到较低维度(embedding)，这样将每个词转成固定维度的向量，句子-文本也就可以看做一个二维矩阵来处理。</p>
<p>（除了直接使用分词结果外，还有基于字符、n-gram等等许多处理方法，这里就不多做介绍了）</p>
<p>其中onehot到embed这一步相当关键，做这个操作的方法也有很多种</p>
<h4 id="预训练词向量"><a href="#预训练词向量" class="headerlink" title="预训练词向量"></a>预训练词向量</h4><p>传统的方法有通过大量语料预训练进行预训练获得对应的词向量，比较常用的方法有：</p>
<ul>
<li>Skipgram</li>
<li>CBOW</li>
<li>Glove</li>
</ul>
<p>三者其实都是基于文本语料，只是任务略有不同，CBOW通过上下文预测目标词，Skipgram通过目标词预测上下文，Glove预测共现矩阵，接近于矩阵分解，有点类似于推荐系统中的LFM的分解矩阵方法。在这种“伪造”的任务上训练到一定程度以后，即可将每个词对应的向量固定下来，使用时作为字典查询使用。</p>
<h4 id="Embedding层"><a href="#Embedding层" class="headerlink" title="Embedding层"></a>Embedding层</h4><p>另外一种做法是在模型开头加入随机初始化的Embedding层，输入使用onehot。在任务训练的过程中同时训练Embedding层，在任务训练的过程中同时训练Embeding层的参数。</p>
<h4 id="句向量-词袋方法"><a href="#句向量-词袋方法" class="headerlink" title="*句向量-词袋方法"></a>*句向量-词袋方法</h4><p>另一种方法是直接将整句或者整篇文章做成一个向量，这种情况主要使用的是词袋加上各种取关键词的技巧，tf-idf、text-rank等，一般在简单的文本分类中会出现，但随着现在面对的NLP问题越来越复杂，已经很少使用这种方案了。</p>
<h4 id="预训练模型作为向量化工具"><a href="#预训练模型作为向量化工具" class="headerlink" title="预训练模型作为向量化工具"></a>预训练模型作为向量化工具</h4><p>NLP中出现了强力的预训练模型之后，使用预训练模型作为向量化工具也成为一种选择。将切到Token的文本输入到预训练模型中，获取倒数N层的隐层向量输出作为token的向量。优点是利用了预训练模型的知识，而且向量中往往隐含上下文信息，使用的效果一般来说会比固定的预训练词向量好很多。</p>
<p>缺点是预训练模型的往往比较大，推断速度比起直接从字典中查预训练词向量或单层embed层来说也慢许多，代码上复杂度也会有所提高。</p>
<p>（这也是我们制作预训练模型服务的原因，算法同学直接通过grpc调用预训练模型获取结果，一方面将预训练模型统一管理，避免不同同学重复加载浪费内存，另一方面保证了模型开发过程中代码的整洁）</p>
<h3 id="常用的NLP预训练模型"><a href="#常用的NLP预训练模型" class="headerlink" title="常用的NLP预训练模型"></a>常用的NLP预训练模型</h3><h4 id="Bert-论文-Github"><a href="#Bert-论文-Github" class="headerlink" title="Bert 论文 Github"></a>Bert <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">论文</a> <a target="_blank" rel="noopener" href="https://github.com/google-research/bert">Github</a></h4><p>Bert是谷歌基于此前的Transformer模型提出的，2017年谷歌在Attention is all you need 这篇文章中提出了一种没有RNN结构、主要依赖于注意力机制的模型Transformer。Transformer由Encoder-Decoder两个主要部分组成，其中主要的技巧是使用了多头的Self-Attention机制（具体Attention机制的原理可以我此前在<a href="https://jonuknownothingsnow.github.io/2019/01/01/Transformer%20%E4%B8%8EAttention%E6%9C%BA%E5%88%B6/">这篇博客</a>写过一下，这里就不重复了)和postion-encode(将位置信息编码到词向量中）。最早用于Seq2Seq任务（翻译、文本摘要、QA等等）</p>
<p>2018年底，谷歌提出了Bert模型， 加入了新的训练技巧，不过本质上并没有对Transformer做太大的变动，主要使用的技巧有：</p>
<ul>
<li>MLM：遮罩语料中的部分单词，令模型预测被遮罩单词的内容</li>
<li>NSP：给出两句话A1与X，判断X是否是A1的下一句（A2），或是无关文本中的其他句子（B1)</li>
</ul>
<p>Bert在结构上相对Transformer并无太大改进，而是通过设计训练任务，巧妙的利用了文本自身的标注属性，从而充分利用了大量无标注的文本，一次在各项任务上都达到了不错的水平。</p>
<h4 id="Roberta-论文-Github"><a href="#Roberta-论文-Github" class="headerlink" title="Roberta 论文  Github"></a>Roberta <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.11692">论文</a>  <a target="_blank" rel="noopener" href="https://github.com/pytorch/fairseq/tree/master/examples/roberta">Github</a></h4><p>2019年7月Facebook推出了Roberta(Robustly optimized BERT approach)，是目前最主流的预训练模型之一，同时也是中文预训练版本较强的模型，哈工大NLP、Clue等多个组织都推出过自己改进版的中文Roberta预训练模型, 在CLUE的<a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUE">榜单</a>上，roberta-wwm甚至要压过albert一筹。roberta主要做的改进有：</p>
<ul>
<li>更大量的数据、更长时间的训练、更大的批次（大力出奇迹，还有一堆训练调参技巧：batch_size、优化器learning_rate等）</li>
<li>修改了MLM任务, 使用动态masking方法，虽然说是动态，只是抽样加大了10倍，本来一次抽样mask之后用四十次，四十次里同一句话的mask是一致的，加大十倍之后平均四次里一句话的mask是一致的，而其他时间里同一句话的mask可能产生变化。</li>
<li>取消了NSP-loss，输入改为FULL-SENTENCES输入，尽可能填满512长度，如果不够会并入多个文本（文本结束会加分隔符）</li>
<li>Byte-Pair Encoding，不针对词或字符， 而是基于统计方法抽取的字节对的encoding</li>
</ul>
<h4 id="ELECTRA-论文-Github"><a href="#ELECTRA-论文-Github" class="headerlink" title="ELECTRA 论文 Github"></a>ELECTRA <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.10555">论文</a> <a target="_blank" rel="noopener" href="https://github.com/google-research/electra">Github</a></h4><p>谷歌和斯坦福于2019年9月推出的ELECTRA(Efficiently Learning an Encoder that Classifies Token Replacements Accurately）是目前几个主流的预训练模型之一，主要的改进点有：</p>
<ul>
<li>RTD任务(Replaced token detection):改进版本的MLM， 采用了类似于GAN的机制，使用一个生成模型去修改原始语料中被遮罩的词，训练判别模型判断具体哪个词被修改过。</li>
</ul>
<h4 id="Albert-论文-Github"><a href="#Albert-论文-Github" class="headerlink" title="Albert 论文 Github"></a>Albert <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.11942">论文</a> <a target="_blank" rel="noopener" href="https://github.com/google-research/albert">Github</a></h4><p>2019年底由谷歌发布的Albert是现在目前最主流的模型版本之一（甚至没有之一）， 目前主流的NLP任务榜单上它都有优异的表现，可以说是Bert的正统继承人（笑）</p>
<ul>
<li><p>Factorized embedding parameterization：有点类似于竞赛中常用的自编码机，理念上也和FM(因子分解机)有些相似。本来Bert自己用的embeding大小是V(字符数）*H(向量维度)，由于V很大，H稍微增大点就会变很大，现在改成先通过一个远小于H的维度E，字符先转化到E维度的向量，再转化到H维度，总参数量变成了（V*E + E*H),大大减少了参数数目。</p>
</li>
<li><p>层共享参数（Cross-layer parameter sharing）：Albert在attention以及前向神经网络(可选)之间共享参数，相对本来每层参数完全独立时减少了参数量，并具备更好的稳定性。</p>
</li>
<li><p>Sentence-Order Prediction     改进版的NSP，原本的训练任务以一篇文章里的上一句A1和下一句A2作为正例， 以A1与另一篇文章里的B1作为负例，这给了模型取巧的机会。新的任务改为以A1-A2作为正例，以A2-A1为负例，要求模型对语义的理解更深</p>
</li>
</ul>
<h4 id="DistilBert-论文-Github"><a href="#DistilBert-论文-Github" class="headerlink" title="DistilBert 论文 Github"></a>DistilBert <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.01108">论文</a> <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/tree/master/examples/distillation">Github</a></h4><p>HuggingFace在2019年10月推出的DistilBert主要是应用了模型蒸馏的技术，在尽可能保证精度的情况下，对模型进行压缩。</p>
<p>模型蒸馏目前的主要方法是利用教师-学生网络，利用大型的教师网络协助复杂度较低的学生网络训练，使得小型的学生网络能够尽量接近教师网络的表现。一般是设计一个较小的学生网络，将原始的标签以及教师网络的输出-隐层输出等作为训练目标，使得学生网络不仅在结果上，而且在中间的隐层上都与教师网络的表现相似，以此达到与教师网络相近的模型水准。</p>
<p>(后续在19年11月DistilBert也放出了针对GPT2、Roberta的蒸馏模型，据称也有不错的表现)</p>
<h4 id="Bert-WWM-论文-Github"><a href="#Bert-WWM-论文-Github" class="headerlink" title="Bert-WWM 论文 Github"></a>Bert-WWM <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.08101">论文</a> <a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-BERT-wwm">Github</a></h4><p>WWM（Whole-Word Masking）是哈工大NLP2019年六月提出的一种针对中文的改进技术，它修改了Bert中随机Mask单字的任务，改为连续Mask一段词，迫使模型通过对上下文的理解来预测，而不是通过高频词组来猜测，能提高模型对语义的理解。</p>
<p>除了用在Bert，这个策略还在robert模型上有应用，推出了robert-wwm，相较原版的robert有所提升。</p>
<p>（19年7月提出的SpanBert虽然是针对英文，但也是采用遮罩连续词组，与WWM在思想上有相似之处。）</p>
<h4 id="此外还有……"><a href="#此外还有……" class="headerlink" title="此外还有……"></a>此外还有……</h4><p>最近一年里比较强势的还有诸如ENIRE, GPT-3，XL-Net等等，这里限于篇幅与主题就不过多介绍了。</p>
<h2 id="预训练模型与平台化"><a href="#预训练模型与平台化" class="headerlink" title="预训练模型与平台化"></a>预训练模型与平台化</h2><p>预训练模型服务只是我们算法平台化工程化的一个小部分，但通过对预训练模型的了解，我们也能够获得更多算法工程化方向的思考，预训练模型作为一种与业务关联相对较小的方面，能让我们从更抽象的角度去思考算法平台化的更多可能性。以下列举几个个人的思考，后面或许可以作为工作或者兴趣开发的方向。</p>
<h3 id="预训练与模型管理"><a href="#预训练与模型管理" class="headerlink" title="预训练与模型管理"></a>预训练与模型管理</h3><p>平台开发工作的推进，模型管理将会是一个重要的平台能力，模型的训练、测试、共享、部署，均需要完善的模型管理体系来支撑。在模型管理层面上，预训练模型和算法同学针对业务开发的模型其实是一样的，我们对预训练模型如果形成了一套完整的存储、更新、管理机制，那这套机制也是可以用在其他业务模型上。</p>
<p>一方面内部模型的保存、注册、查找、版本管理机制需要重视，另一方面对外接口的易用性也要考虑。</p>
<h5 id="值得借鉴的Transformers"><a href="#值得借鉴的Transformers" class="headerlink" title="值得借鉴的Transformers"></a>值得借鉴的Transformers</h5><p>目前许多公司、组织都在做模型仓库模型、模型管理的方案，做的最早的应该是谷歌的TF-hub，最近pytorch、paddlepaddle也有了自己的model-zoo。但是在Transformer这个细分方向，做的最好，最为社区所接受的，应该还是HuggingFace的Transformers</p>
<p>Transformers提供了统一的模型仓库，开发者只要遵循一定的格式，就可以将自己的预训练模型注册到它的仓库。目前上面不仅有主流的英文预训练模型，中文方面的主流模型例如上文提到的albert、roberta-wwm等，作者也早已支持Transformers进行调用。</p>
<p>使用者可以像拉取docker镜像一样快速获取到指定的预训练模型版本，在十行代码左右，就可以运行预训练模型执行QA、MLM、摘要等任务，也可以直接用于文本的向量化。</p>
<p>此外Transformer还同时支持Pytorch与TF两种后端，在上层提供了统一的使用接口。作为模型管理与加载的工具，值得学习借鉴。</p>
<h3 id="任务调度有以及模型训练的进一步解耦"><a href="#任务调度有以及模型训练的进一步解耦" class="headerlink" title="任务调度有以及模型训练的进一步解耦"></a>任务调度有以及模型训练的进一步解耦</h3><p>目前一个比较常见的模型生命周期分解方式可能是 数据-训练-测试-部署。但是当我们回过头来看刚才的多个主流预训练模型时，我们发现其实许多不同的模型方案，可能并没有对模型结构本身做什么改动，有的只是改动了训练的任务类型，例如NSP改SOP， MLM改动态mask，改wwm等。从工程的角度来看，这些训练任务是否有可能与模型结构进行进一步的解耦，从而独立成可复用的组件，可以考虑一下。</p>
<p>初步看任务类型主要还是对数据处理上有所不同，是一个怎么从原始数据里取出（X，Y）的过程，如果能形成规范，统一交付给模型训练用的(X,Y)格式，理论上是可能做到解耦任务这一环的。 如果形成这个标准，那么大家可以在同一套标准下开发自己的任务组件，也可以复用他人的任务组件，可能会在算法开发上带来较大的便利。</p>
<p>另外一个难点在于，NLP的任务、CV的任务以及数据挖掘方面的任务，各自会有较大差别，这个交付标准必须要能够兼容，同时需要结合平台的数据存放、管理、读取模式进行考量，是颇有挑战性的工作。</p>
<h3 id="模型Ensemble方案与模型部署"><a href="#模型Ensemble方案与模型部署" class="headerlink" title="模型Ensemble方案与模型部署"></a>模型Ensemble方案与模型部署</h3><p>在许多算法比赛中以及一些榜单中，要夺得较好的名次，一般都需要复合多种模型，以达到更好的效果。现实业务中往往不会这样做，一个原因当然是因为开发上的难度以及推理速度上的考量，另一个重要的原因也是因为多个模型的复合可能给线上模型的部署造成困难。</p>
<p>然而随着我们平台化的不断深入推进，多个不同模型的部署，以及组合进行推断所需要的成本可能不断降低。我们可以部署多个模型，提供规范的推理接口，由业务系统组合调用。某些情况下业务系统可以同时并发的请求多个模型的结果，并对结果进行整合形成最终的返回值。又或者通过先请求预训练模型拿到向量， 再将向量发送给业务模型获取最终结果，以实现模型的串行。</p>
<p>只要我们在部署上有一套合理的方案，模型Ensemble或许能在实际业务中给我们带来益处。</p>
<h3 id="等等等等……"><a href="#等等等等……" class="headerlink" title="等等等等……"></a>等等等等……</h3><p>此外例如分布式训练，以参数服务器模式为例，在K8S上我们是否可能做到，只要保证Param-Server存活，动态伸缩Workers进行训练。对于环形的分布式训练又如何解决，有价值的有待解决的命题实在是很多。</p>
<p>今天就先到这里了，靴靴。</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>王大炮
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://jonuknownothingsnow.github.io/2020/08/15/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%9C%8D%E5%8A%A1%E5%88%86%E4%BA%AB/" title="谐门武学-NLP预训练模型服务分享">https://jonuknownothingsnow.github.io/2020/08/15/NLP预训练模型服务分享/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Bert/" rel="tag"># Bert</a>
              <a href="/tags/Albert/" rel="tag"># Albert</a>
              <a href="/tags/Roberta/" rel="tag"># Roberta</a>
              <a href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" rel="tag"># 预训练模型</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/06/13/%E9%9B%B6%E6%95%A3%E8%AE%B0%E5%BD%9520200613/" rel="prev" title="零散记录:20200613">
      <i class="fa fa-chevron-left"></i> 零散记录:20200613
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">什么是预训练模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E9%A2%86%E5%9F%9F%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">自然语言领域的预训练模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-number">2.1.</span> <span class="nav-text">向量化问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F"><span class="nav-number">2.1.1.</span> <span class="nav-text">预训练词向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Embedding%E5%B1%82"><span class="nav-number">2.1.2.</span> <span class="nav-text">Embedding层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%A5%E5%90%91%E9%87%8F-%E8%AF%8D%E8%A2%8B%E6%96%B9%E6%B3%95"><span class="nav-number">2.1.3.</span> <span class="nav-text">*句向量-词袋方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BD%9C%E4%B8%BA%E5%90%91%E9%87%8F%E5%8C%96%E5%B7%A5%E5%85%B7"><span class="nav-number">2.1.4.</span> <span class="nav-text">预训练模型作为向量化工具</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.</span> <span class="nav-text">常用的NLP预训练模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Bert-%E8%AE%BA%E6%96%87-Github"><span class="nav-number">2.2.1.</span> <span class="nav-text">Bert 论文 Github</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Roberta-%E8%AE%BA%E6%96%87-Github"><span class="nav-number">2.2.2.</span> <span class="nav-text">Roberta 论文  Github</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ELECTRA-%E8%AE%BA%E6%96%87-Github"><span class="nav-number">2.2.3.</span> <span class="nav-text">ELECTRA 论文 Github</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Albert-%E8%AE%BA%E6%96%87-Github"><span class="nav-number">2.2.4.</span> <span class="nav-text">Albert 论文 Github</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DistilBert-%E8%AE%BA%E6%96%87-Github"><span class="nav-number">2.2.5.</span> <span class="nav-text">DistilBert 论文 Github</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bert-WWM-%E8%AE%BA%E6%96%87-Github"><span class="nav-number">2.2.6.</span> <span class="nav-text">Bert-WWM 论文 Github</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A4%E5%A4%96%E8%BF%98%E6%9C%89%E2%80%A6%E2%80%A6"><span class="nav-number">2.2.7.</span> <span class="nav-text">此外还有……</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%B9%B3%E5%8F%B0%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text">预训练模型与平台化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%A8%A1%E5%9E%8B%E7%AE%A1%E7%90%86"><span class="nav-number">3.1.</span> <span class="nav-text">预训练与模型管理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%80%BC%E5%BE%97%E5%80%9F%E9%89%B4%E7%9A%84Transformers"><span class="nav-number">3.1.0.1.</span> <span class="nav-text">值得借鉴的Transformers</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%9C%89%E4%BB%A5%E5%8F%8A%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E8%A7%A3%E8%80%A6"><span class="nav-number">3.2.</span> <span class="nav-text">任务调度有以及模型训练的进一步解耦</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8BEnsemble%E6%96%B9%E6%A1%88%E4%B8%8E%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2"><span class="nav-number">3.3.</span> <span class="nav-text">模型Ensemble方案与模型部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%89%E7%AD%89%E7%AD%89%E7%AD%89%E2%80%A6%E2%80%A6"><span class="nav-number">3.4.</span> <span class="nav-text">等等等等……</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">王大炮</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">105</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">王大炮</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
