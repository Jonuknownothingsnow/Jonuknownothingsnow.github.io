<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jonuknownothingsnow.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="谐门武学：特征选择与Null Importance特征工程是我们在做机器学习工作中的重要一环，根据我的工作经验，其实特征工程可以分为两个部分，一个是特征生产，另一个是特征选择，今天我们就针对特征选择来进行一下探讨，并给大家介绍一个我在比赛中学到的新的把戏。想看这个把戏的可以直接跳转到[使用Null importance 进行特征选择](#使用Null importance 进行特征选择) 特征选择">
<meta property="og:type" content="article">
<meta property="og:title" content="谐门武学：特征选择与Null Importance">
<meta property="og:url" content="https://jonuknownothingsnow.github.io/2018/10/21/%E8%B0%90%E9%97%A8%E6%AD%A6%E5%AD%A6%EF%BC%9A%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8ENull%20Importance/index.html">
<meta property="og:site_name" content="火力支援">
<meta property="og:description" content="谐门武学：特征选择与Null Importance特征工程是我们在做机器学习工作中的重要一环，根据我的工作经验，其实特征工程可以分为两个部分，一个是特征生产，另一个是特征选择，今天我们就针对特征选择来进行一下探讨，并给大家介绍一个我在比赛中学到的新的把戏。想看这个把戏的可以直接跳转到[使用Null importance 进行特征选择](#使用Null importance 进行特征选择) 特征选择">
<meta property="og:locale">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNbRwgy1fwfz3gucfaj30tk0s0wpj.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tNbRwly1fwfxtp9hb6j30lj0arjsf.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNbRwly1fwfxvfh63fj30l70art9r.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tNbRwly1fwfxyuhu70j30l70asdgx.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tNbRwly1fwfy7vb3fhj30vs0vs0ug.jpg">
<meta property="article:published_time" content="2018-10-20T16:00:00.000Z">
<meta property="article:modified_time" content="2020-08-18T06:31:54.694Z">
<meta property="article:author" content="王大炮">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="谐门武学">
<meta property="article:tag" content="特征工程">
<meta property="article:tag" content="特征选择">
<meta property="article:tag" content="实战技巧">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ws3.sinaimg.cn/large/006tNbRwgy1fwfz3gucfaj30tk0s0wpj.jpg">

<link rel="canonical" href="https://jonuknownothingsnow.github.io/2018/10/21/%E8%B0%90%E9%97%A8%E6%AD%A6%E5%AD%A6%EF%BC%9A%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8ENull%20Importance/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>谐门武学：特征选择与Null Importance | 火力支援</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火力支援</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://jonuknownothingsnow.github.io/2018/10/21/%E8%B0%90%E9%97%A8%E6%AD%A6%E5%AD%A6%EF%BC%9A%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8ENull%20Importance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="王大炮">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火力支援">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          谐门武学：特征选择与Null Importance
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-21 00:00:00" itemprop="dateCreated datePublished" datetime="2018-10-21T00:00:00+08:00">2018-10-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-18 14:31:54" itemprop="dateModified" datetime="2020-08-18T14:31:54+08:00">2020-08-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95-%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">算法/模型</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="谐门武学：特征选择与Null-Importance"><a href="#谐门武学：特征选择与Null-Importance" class="headerlink" title="谐门武学：特征选择与Null Importance"></a>谐门武学：特征选择与Null Importance</h1><p>特征工程是我们在做机器学习工作中的重要一环，根据我的工作经验，其实特征工程可以分为两个部分，一个是特征生产，另一个是特征选择，今天我们就针对特征选择来进行一下探讨，并给大家介绍一个我在比赛中学到的新的把戏。想看这个把戏的可以直接跳转到[使用Null importance 进行特征选择](#使用Null importance 进行特征选择)</p>
<h2 id="特征选择的常见方法"><a href="#特征选择的常见方法" class="headerlink" title="特征选择的常见方法"></a>特征选择的常见方法</h2><p>特征选择是机器学习里面一个很老的话题了，常见的特征选择方法我觉得可以归为两大类，一类是基于贪心算法的，接近于暴力搜索。另一类是基于具体的机器学习模型。</p>
 <a id="more"></a>

<h3 id="基于贪心算法的特征选择"><a href="#基于贪心算法的特征选择" class="headerlink" title="基于贪心算法的特征选择"></a>基于贪心算法的特征选择</h3><p>基于贪心算法的特征选择也有两种方式，一种是从零到全部，另一种是从全部到零，其实效果都差不多。前者被称为包裹式，后者被称为过滤式。简单来说包裹式就是：</p>
<ol>
<li>随机选中一个特征，作为特征组F跑一遍模型，记分数$s_1$</li>
<li>随机选中一个未被选中过的特征与F一起跑一遍模型，记分数$s_2$。</li>
<li>如果$s_2 &gt; s_1+b$,将新的特征加入到F中，若否，不加入。这里的b为可调节的阈值，通常为零。</li>
<li>重复2~3步骤，直到所有特征都被选择过，输出最终的特征组F。</li>
</ol>
<p>过滤式其实就是一模一样的思路，只不过从大到小而已：</p>
<ol>
<li>选取全部特征，作为特征组F跑一遍模型，记分数$s_1$</li>
<li>随机选中F中的一个特征屏蔽，跑一遍模型，记分数$s_2$。</li>
<li>如果$s_2 &gt; s_1-b$,将该特征从F中去除，若否，不则不去除。</li>
<li>重复2~3步骤，直到所有特征都被测试过，输出最终的特征组F。</li>
</ol>
<p>这就是非常简单常用的贪心算法特征性选择。</p>
<h3 id="基于模型的特征选择"><a href="#基于模型的特征选择" class="headerlink" title="基于模型的特征选择"></a>基于模型的特征选择</h3><p>另外一种比较常用的特征选择方法，是基于特定机器学习模型的特性的，使用较多的是基于L1正则化的嵌入式特征选择，和基于随机森林的feature importance。</p>
<h4 id="使用L1正则化进行特征选择"><a href="#使用L1正则化进行特征选择" class="headerlink" title="使用L1正则化进行特征选择"></a>使用L1正则化进行特征选择</h4><p>对于L1正则化，网上有很多详细解说的资料和图片，我这里就简单的讲下我的理解方式好了。L1正则化和L2正则化都是防止过拟合的方法，它们作为量化的惩罚项来阻止你将自己的模型变得越来越复杂。</p>
<p>举例来说假如我们要用线性回归来做一个理论，$y = k_1x_1+k_2x_2+k_3x_3$,这个式子就是我们理论假设，假设当然是在同样解释力下，越简单越好，L1正则化通过$|k_1|+|k_2|+|k_3|$来衡量复杂度，L2正则化通过$k_1^2+k_2^2+k_3^2)$来衡量。</p>
<p>可以看出，L2正则化中（1，0，0）这一参数组合的复杂度是比（0.5，0.5，0）的复杂度要高的，但是在L1正则化中（1，0，0）的复杂度和（0.5，0.5，0）、（0.4，0.3，0.3）是一样的，所以使用L1正则化时，更容易使一些不重要的特征的系数降为零，起到特征选择的效果。</p>
<p>因此与其他的特征选择方式不同，它本身并不会输出一个特征组，而是在训练的过程中把不重要的特征的系数调成零。当然如果你是在使用线性回归或者逻辑斯蒂回归之类的算法的话，可以输出各个特征最终的系数，来确认哪些特征被剔除了。</p>
<p>实际上L1、L2正则化的应用相当广泛，从线性回归（使用L1正则化的线性回归即lasso回归，使用L2的即是岭回归）到xgboost，lightgbm中，都可以使用l1、l2正则化。它一般作为loss的一个附加项来在训练过程中进行控制：<br>$$<br>loss’ = loss + \alpha(|k_1|+|k_2|+|k_3|)+\lambda(k_1^2+k_2^2+k_3^2)<br>$$<br>其中α和λ分别是L1、L2正则化的系数，用来控制其“惩罚力度”</p>
<h4 id="使用Feature-Importance进行特征选择"><a href="#使用Feature-Importance进行特征选择" class="headerlink" title="使用Feature Importance进行特征选择"></a>使用Feature Importance进行特征选择</h4><p>feature importance是随机森林的一个特色技能，随机森林会训练多个决策树，不同的决策树之间相互独立，并且在训练每个决策树时，仅使用部分特征与数据（关于随机森林想了解更多？可以参看<a href="https://jonuknownothingsnow.github.io/2018/04/05/%E8%B0%90%E9%97%A8%E7%AE%97%E6%B3%95%EF%BC%9A%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95%E4%BB%AC/">谐门算法：集成算法们</a>中的bagging与随机森林一节，决策树相关可以看<a href="https://jonuknownothingsnow.github.io/2018/04/05/%E8%B0%90%E9%97%A8%E7%AE%97%E6%B3%95%EF%BC%9A%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95%E4%BB%AC/">谐门算法：手写智障决策树</a>），这一特性得通过统计每个特征在不同决策树中的表现来为特征打分提供了可能。</p>
<p>在sklearn的决策树里提供了一个函数来输出feature importance,可以这样使用:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">clf = RandomForestClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练完成！</span></span><br><span class="line">clf.fit(X,y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出feature importance</span></span><br><span class="line">split = clf.feature_importance(importance_type=<span class="string">&#x27;split&#x27;</span>)</span><br><span class="line">gain = clf.feature_importance(importance_type=<span class="string">&quot;gain&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>上面我们看到随机森林可以输出两种feature importance，一种是split，一种是gain。</p>
<p>split就是特征在所有决策树中被用来分割的总次数。</p>
<p>gain就是特征在所有决策树种被用来分割后带来的增益(gain)总和（关于增益(gain)是决策树原理中的一个概念，可以参看<a href="https://jonuknownothingsnow.github.io/2018/04/05/%E8%B0%90%E9%97%A8%E7%AE%97%E6%B3%95%EF%BC%9A%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95%E4%BB%AC/">谐门算法：手写智障决策树</a>中关于分割与信息增益的部分）</p>
<p>两者都可以作为评价特征有用程度的指标。</p>
<p>不过这个方法当然也是有缺点的，主要一个就是，我们拿到一个feature importance之后，它只是一个数值，我们怎么知道数值大于多少才是有用的，数值小于多少就应该抛弃呢？我们接下来要讲的Null Importance方法就是试图解决这一问题。</p>
<h2 id="使用Null-importance-进行特征选择"><a href="#使用Null-importance-进行特征选择" class="headerlink" title="使用Null importance 进行特征选择"></a>使用Null importance 进行特征选择</h2><p>有的时候模型其实很蠢，很多根本就和目标没有关联的东西，它也可以瞎扯一通和目标关联上，这种虚假的关联到测试集上当然就扑街了，这就是我们常说的过拟合。那么回到我们一开始的问题，如何在feature importance中画出一条线，来区分这个特征是否有用呢？</p>
<p>思想很简单，就是将特征分数与随机假特征的分数（即Null Importance）进行对比，假如特征的分数并不能明显超过null importance，那么证明这个特征是一个无用的特征。</p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>具体的实现方法可以有很多，我们这里介绍的一种来自于知名Kaggle Master级选手olivier的kernel(<a target="_blank" rel="noopener" href="https://www.kaggle.com/ogrellier/feature-selection-with-null-importances">Feature Selection with Null Importances</a>)，主要代码均一致，我做了些小调整便于演示。</p>
<p>首先我们随手选一个数据集来作为演示用的数据集，额，鸢尾花之前用过了，波士顿房价就你了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston,load_iris</span><br><span class="line">boston = load_boston()</span><br><span class="line">data = pd.DataFrame(boston[<span class="string">&quot;data&quot;</span>])</span><br><span class="line"><span class="comment">#我们做两个随机的噪声项进去，看看噪声项的表现是怎么样的</span></span><br><span class="line">data[<span class="string">&quot;noise_1&quot;</span>] = np.random.normal(size=len(X))<span class="comment">#正态分布</span></span><br><span class="line">data[<span class="string">&quot;noise_2&quot;</span>] = np.random.randint(<span class="number">100</span>,size=len(X))<span class="comment">#随机整数</span></span><br><span class="line">data[<span class="string">&quot;target&quot;</span>] = boston[<span class="string">&quot;target&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>接下来是olivier的实现思路，他的实现不是添加新的噪声项，而是把原特征与目标的对应关系打乱。这样的好处是保存了特征本身的数值分布，同一个特征与打乱之后的自身比对，更有参考价值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature_importances</span>(<span class="params">data, shuffle, seed=None</span>):</span></span><br><span class="line">    <span class="comment"># 特征</span></span><br><span class="line">    train_features = [f <span class="keyword">for</span> f <span class="keyword">in</span> data <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;target&#x27;</span>]]</span><br><span class="line">   </span><br><span class="line">    y = data[<span class="string">&#x27;target&#x27;</span>].copy()</span><br><span class="line">    <span class="comment">#在造null importance时打乱</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># 为了打乱而不影响原始数据，采用了.copy().sample(frac=1.0)这种有点奇怪的做法</span></span><br><span class="line">        y = data[<span class="string">&#x27;target&#x27;</span>].copy().sample(frac=<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用lgb的随机森林模式，据说会比sklearn的随机森林快点</span></span><br><span class="line">    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=<span class="literal">False</span>, silent=<span class="literal">True</span>)</span><br><span class="line">    lgb_params = &#123;</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;regression_l2&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;rf&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.623</span>,</span><br><span class="line">        <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.7</span>,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">127</span>,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">&#x27;seed&#x27;</span>: seed,</span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;n_jobs&#x27;</span>: <span class="number">4</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#训练</span></span><br><span class="line">    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get feature importances</span></span><br><span class="line">    imp_df = pd.DataFrame()</span><br><span class="line">    imp_df[<span class="string">&quot;feature&quot;</span>] = list(train_features)</span><br><span class="line">    imp_df[<span class="string">&quot;importance_gain&quot;</span>] = clf.feature_importance(importance_type=<span class="string">&#x27;gain&#x27;</span>)</span><br><span class="line">    imp_df[<span class="string">&quot;importance_split&quot;</span>] = clf.feature_importance(importance_type=<span class="string">&#x27;split&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> imp_df</span><br><span class="line"></span><br><span class="line"><span class="comment">#我们先来看下原始的feature_importance好了</span></span><br><span class="line">actual_imp = get_feature_importances(data=data,shuffle=<span class="literal">False</span>)</span><br><span class="line">actual_imp.sort_values(<span class="string">&quot;importance_gain&quot;</span>,ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fwfz3gucfaj30tk0s0wpj.jpg" alt="image-20181021162750724"></p>
<p>两个噪音项竟然不是倒数第一，后面那几个特征也算有实力了。接下来我们制造一批Null Importance</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">null_imp_df = pd.DataFrame()</span><br><span class="line">nb_runs = <span class="number">80</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time()</span><br><span class="line">dsp = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_runs):</span><br><span class="line">    <span class="comment"># 获取当前轮feature impotance</span></span><br><span class="line">    imp_df = get_feature_importances(data=data, shuffle=<span class="literal">True</span>)</span><br><span class="line">    imp_df[<span class="string">&#x27;run&#x27;</span>] = i + <span class="number">1</span> </span><br><span class="line">    <span class="comment"># 加到合集上去</span></span><br><span class="line">    null_imp_df = pd.concat([null_imp_df, imp_df], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 擦除旧信息</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(len(dsp)):</span><br><span class="line">        print(<span class="string">&#x27;\b&#x27;</span>, end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 显示当前轮信息</span></span><br><span class="line">    spent = (time.time() - start) / <span class="number">60</span></span><br><span class="line">    dsp = <span class="string">&#x27;Done with %4d of %4d (Spent %5.1f min)&#x27;</span> % (i + <span class="number">1</span>, nb_runs, spent)</span><br><span class="line">    print(dsp, end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>跑好了，选个特征来看看，就挑12号吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_distributions(actual_imp_df_=actual_imp, null_imp_df_=null_imp_df, feature_=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fwfxtp9hb6j30lj0arjsf.jpg"></p>
<p>证明12号这个特征还是挺有用的，实力超过打乱后的自己很多，我们来挑个辣鸡的，9号，就你了。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fwfxvfh63fj30l70art9r.jpg"></p>
<p>9号你这个跟打乱了的null importance根本就没差别嘛。我们再来看看noise_1吧</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fwfxyuhu70j30l70asdgx.jpg"></p>
<p>到这里其实我们已经知道具体怎么使用null importance来鉴别特征了，你可以设置一个指标来简单的划分出特征，具体的方式可以有很多，比方说是否大于平均值啦，是否大于最大值啦太多了，看自己理解。olivier的方法是使用真实的importance分数和自己null importance的75%分位数（即四分之三位数，类似于中位数，50%分位数就是中位数）进行对比，计算出一个新的分数。公式是：<br>$$<br>score = \log((1+actual_importance)/(1+null_importance_75))<br>$$<br>用这个score来表示特征的重要性，0就代表和随机无关联数据没啥区别，数字越大代表重要性越高。代码实现如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">feature_scores = []</span><br><span class="line"><span class="keyword">for</span> _f <span class="keyword">in</span> actual_imp[<span class="string">&#x27;feature&#x27;</span>].unique():</span><br><span class="line">    f_null_imps_gain = null_imp_df.loc[null_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_gain&#x27;</span>].values</span><br><span class="line">    f_act_imps_gain = actual_imp.loc[actual_imp[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_gain&#x27;</span>].mean()</span><br><span class="line">    gain_score = np.log(<span class="number">1e-10</span> + f_act_imps_gain / (<span class="number">1</span> + np.percentile(f_null_imps_gain, <span class="number">75</span>)))  <span class="comment"># Avoid didvide by zero</span></span><br><span class="line">    f_null_imps_split = null_imp_df.loc[null_imp_df[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_split&#x27;</span>].values</span><br><span class="line">    f_act_imps_split = actual_imp.loc[actual_imp[<span class="string">&#x27;feature&#x27;</span>] == _f, <span class="string">&#x27;importance_split&#x27;</span>].mean()</span><br><span class="line">    split_score = np.log(<span class="number">1e-10</span> + f_act_imps_split / (<span class="number">1</span> + np.percentile(f_null_imps_split, <span class="number">75</span>)))  <span class="comment"># Avoid didvide by zero</span></span><br><span class="line">    feature_scores.append((_f, split_score, gain_score))</span><br><span class="line"></span><br><span class="line">scores_df = pd.DataFrame(feature_scores, columns=[<span class="string">&#x27;feature&#x27;</span>, <span class="string">&#x27;split_score&#x27;</span>, <span class="string">&#x27;gain_score&#x27;</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">gs = gridspec.GridSpec(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># Plot Split importances</span></span><br><span class="line">ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;split_score&#x27;</span>, y=<span class="string">&#x27;feature&#x27;</span>, data=scores_df.sort_values(<span class="string">&#x27;split_score&#x27;</span>, ascending=<span class="literal">False</span>).iloc[<span class="number">0</span>:<span class="number">70</span>], ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Feature scores wrt split importances&#x27;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"><span class="comment"># Plot Gain importances</span></span><br><span class="line">ax = plt.subplot(gs[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;gain_score&#x27;</span>, y=<span class="string">&#x27;feature&#x27;</span>, data=scores_df.sort_values(<span class="string">&#x27;gain_score&#x27;</span>, ascending=<span class="literal">False</span>).iloc[<span class="number">0</span>:<span class="number">70</span>], ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Feature scores wrt gain importances&#x27;</span>, fontweight=<span class="string">&#x27;bold&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>

<p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fwfy7vb3fhj30vs0vs0ug.jpg"></p>
<h3 id="杂七杂八"><a href="#杂七杂八" class="headerlink" title="杂七杂八"></a>杂七杂八</h3><p>其实如果嫌烦的话，跟我一样加几个噪音项进去，充当null importance作为特征的参考物，我觉得也是挺可行的。</p>
<h2 id="靴靴"><a href="#靴靴" class="headerlink" title="靴靴"></a>靴靴</h2><p>今天就到这里啦，靴靴。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E8%B0%90%E9%97%A8%E6%AD%A6%E5%AD%A6/" rel="tag"># 谐门武学</a>
              <a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag"># 特征工程</a>
              <a href="/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/" rel="tag"># 特征选择</a>
              <a href="/tags/%E5%AE%9E%E6%88%98%E6%8A%80%E5%B7%A7/" rel="tag"># 实战技巧</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/10/14/%E8%B0%90%E9%97%A8%E7%AE%97%E6%B3%95%EF%BC%9ADBSCAN%E5%8E%9F%E7%90%86/" rel="prev" title="谐门算法:DBSCAN原理">
      <i class="fa fa-chevron-left"></i> 谐门算法:DBSCAN原理
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/12/01/%E8%B0%90%E9%97%A8%E6%AD%A6%E5%AD%A6%EF%BC%9A%E4%BD%BF%E7%94%A8DOCKER%E6%90%AD%E5%BB%BAtensorflow%20%E7%9A%84gpu%E7%8E%AF%E5%A2%83/" rel="next" title="谐门武学:Tensorflow GPU踩坑记录">
      谐门武学:Tensorflow GPU踩坑记录 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B0%90%E9%97%A8%E6%AD%A6%E5%AD%A6%EF%BC%9A%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8ENull-Importance"><span class="nav-number">1.</span> <span class="nav-text">谐门武学：特征选择与Null Importance</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95"><span class="nav-number">1.1.</span> <span class="nav-text">特征选择的常见方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-number">1.1.1.</span> <span class="nav-text">基于贪心算法的特征选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-number">1.1.2.</span> <span class="nav-text">基于模型的特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8L1%E6%AD%A3%E5%88%99%E5%8C%96%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">使用L1正则化进行特征选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Feature-Importance%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">使用Feature Importance进行特征选择</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Null-importance-%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-number">1.2.</span> <span class="nav-text">使用Null importance 进行特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.2.1.</span> <span class="nav-text">实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB"><span class="nav-number">1.2.2.</span> <span class="nav-text">杂七杂八</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%B4%E9%9D%B4"><span class="nav-number">1.3.</span> <span class="nav-text">靴靴</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">王大炮</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">105</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">王大炮</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
